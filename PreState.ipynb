{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc9d72-6cf8-4d6b-8d13-11788c61d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2adac-cff8-4be4-b602-66d13334efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/'Colab_Notebooks'/mmser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa66f74-67d5-49e2-8aec-13eed1a748e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[torch]\n",
    "!pip install tensorboardX\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0faec3f-1899-4c54-8929-bad602beed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvggish import vggish, vggish_input\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from transformers import BertForSequenceClassification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "currentSecond= datetime.now().second\n",
    "currentMinute = datetime.now().minute\n",
    "currentHour = datetime.now().hour\n",
    "\n",
    "currentDay = datetime.now().day\n",
    "currentMonth = datetime.now().month\n",
    "currentYear = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a5200-df41-427b-913d-099f6a879917",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dirname        : path that need to be searched\n",
    "ret                : files in the dirname (recursive)\n",
    "list_avoid_dir : dirname need to be skipped\n",
    "usage           : \n",
    "    list_files = []\n",
    "    file_search(dirname, list_files):   \n",
    "'''\n",
    "def file_search(dirname, ret, list_avoid_dir=[]):\n",
    "    \n",
    "    filenames = os.listdir(dirname)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "\n",
    "        if os.path.isdir(full_filename) :\n",
    "            if full_filename.split('/')[-1] in list_avoid_dir:\n",
    "                continue\n",
    "            else:\n",
    "                file_search(full_filename, ret, list_avoid_dir)\n",
    "            \n",
    "        else:\n",
    "            ret.append(full_filename)          \n",
    "\n",
    "            \n",
    "\n",
    "'''\n",
    "filename : filename (inc. path) that will be inspected\n",
    "'''\n",
    "def find_encoding(filename):\n",
    "    rawdata = open(filename, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding']    \n",
    "    return charenc\n",
    "            \n",
    "'''\n",
    "dir_name : dir_name (inc. path) that will be created ( full-path name )\n",
    "'''\n",
    "def create_folder(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e3c45-ce75-4ba4-89d9-2b94b5028f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trans(list_in_file, out_file):\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    for in_file in list_in_file:\n",
    "        cnt = 0\n",
    "        \n",
    "        with open(in_file, 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "\n",
    "        with open(out_file, 'a') as f2:\n",
    "\n",
    "            csv_writer = csv.writer(f2)\n",
    "            lines = sorted(lines)                  # sort based on first element\n",
    "            \n",
    "            for line in lines:\n",
    "\n",
    "                name = line.split(':')[0].split(' ')[0].strip()\n",
    "                \n",
    "                # unwanted case \n",
    "                if name[:3] != 'Ses':             # noise transcription such as reply  M: sorry\n",
    "                    continue\n",
    "                elif name[-3:-1] == 'XX':        # we don't have matching pair in label\n",
    "                    continue\n",
    "                trans = line.split(':')[1].strip()\n",
    "                \n",
    "                cnt += 1\n",
    "                csv_writer.writerow([name, trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451084a-f11a-490d-96ef-0f9a750507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [schema] ID, transcriptions [csv]\n",
    "\n",
    "list_files = []\n",
    "\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "\n",
    "    path = 'data/IEMOCAP_full_release/' + sess_name + '/dialog/transcriptions/'\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "\n",
    "    print (sess_name + \", #sum files: \" + str(len(list_files)))\n",
    "\n",
    "extract_trans(list_files, 'processed_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71653533-f3e3-4683-af05-2343d62b47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents of csv file\n",
    "file = pd.read_csv(\"processed_trans.csv\")\n",
    "  \n",
    "# adding header\n",
    "headerList = ['sessionID', 'text']\n",
    "  \n",
    "# converting data frame to csv\n",
    "file.to_csv(\"processed_trans_head.csv\", header=headerList, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d3c39-97df-4055-bca1-b88e8793e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_category = [\n",
    "                'ang',\n",
    "                'hap',\n",
    "                'sad',\n",
    "                'neu',\n",
    "                'fru',\n",
    "                'exc',\n",
    "                'fea',\n",
    "                'sur',\n",
    "                'dis',\n",
    "                'oth',\n",
    "                'xxx'\n",
    "                ]\n",
    "\n",
    "category = {}\n",
    "for c_type in list_category:\n",
    "    if category.__contains__(c_type):\n",
    "        pass\n",
    "    else:\n",
    "        category[c_type] = len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff6bb3-a08f-4db5-a2ec-d1556043ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_category(lines):\n",
    "    is_target = True\n",
    "    \n",
    "    id = ''\n",
    "    c_label = ''\n",
    "    list_ret = []\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        if is_target == True:\n",
    "            \n",
    "            try:\n",
    "                id          = line.split('\\t')[1].strip()  #  extract ID\n",
    "                c_label  = line.split('\\t')[2].strip()  #  extract category\n",
    "                if not category.__contains__(c_label):\n",
    "                    print(\"ERROR nokey \", c_label)\n",
    "                    sys.exit()\n",
    "                \n",
    "                list_ret.append( [id, c_label] )\n",
    "                is_target = False\n",
    "\n",
    "            except:\n",
    "                print(\"ERROR \", line)\n",
    "                sys.exit()\n",
    "        \n",
    "        else:\n",
    "            if line == '\\n':\n",
    "                is_target = True\n",
    "        \n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4cd04-0517-4e8e-8210-df647b7a5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(list_in_file, out_file) :\n",
    "    id = ''\n",
    "    lines = []\n",
    "    list_ret = []\n",
    "    \n",
    "    for in_file in list_in_file:\n",
    "        \n",
    "        with open(in_file, 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "            lines = lines[2:]                           # remove head\n",
    "            list_ret = find_category(lines)\n",
    "            \n",
    "        list_ret = sorted(list_ret)                   # sort based on first element\n",
    "    \n",
    "        with open(out_file, 'a') as f2:\n",
    "            csv_writer = csv.writer(f2)\n",
    "            csv_writer.writerows(list_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a13d5-99e8-4e28-8834-d5c8e67e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [schema] ID, label [csv]\n",
    "\n",
    "list_files = []\n",
    "list_avoid_dir = ['Attribute', 'Categorical', 'Self-evaluation']\n",
    "\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "\n",
    "    path = 'data/IEMOCAP_full_release/' + sess_name + '/dialog/EmoEvaluation/'\n",
    "    file_search(path, list_files, list_avoid_dir)\n",
    "    list_files = sorted(list_files)\n",
    "\n",
    "    print(sess_name + \", #sum files: \" + str(len(list_files)))\n",
    "\n",
    "extract_labels(list_files, \"processed_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565dc88-9a24-45bf-8cf9-293153b798e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents of csv file\n",
    "file = pd.read_csv(\"processed_labels.csv\")\n",
    "  \n",
    "# adding header\n",
    "headerList = ['sessionID', 'label']\n",
    "  \n",
    "# converting data frame to csv\n",
    "file.to_csv(\"processed_labels_head.csv\", header=headerList, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5b862-96fb-4d52-955a-385a388bfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.read_csv('processed_labels_head.csv')\n",
    "dfl.loc[dfl[\"label\"] == \"ang\", \"label\"] = 0\n",
    "dfl.loc[dfl[\"label\"] == \"hap\", \"label\"] = 1\n",
    "dfl.loc[dfl[\"label\"] == \"exc\", \"label\"] = 1\n",
    "dfl.loc[dfl[\"label\"] == \"sad\", \"label\"] = 2\n",
    "dfl.loc[dfl[\"label\"] == \"neu\", \"label\"] = 3\n",
    "dfl.loc[dfl[\"label\"] == \"fru\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"fea\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"sur\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"dis\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"oth\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"xxx\", \"label\"] = -1\n",
    "dfl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ca0ab-a865-4ae8-8445-6ff5f9f23131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.to_csv(\"processed_digital_labels_head.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22cbe0-97cb-4d09-8657-4fff2f412229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading two csv files\n",
    "data1 = pd.read_csv('processed_trans_head.csv')\n",
    "data2 = pd.read_csv('processed_digital_labels_head.csv')\n",
    "  \n",
    "# using merge function by setting how='inner'\n",
    "translabels = pd.merge(data1, data2, \n",
    "                   on='sessionID', \n",
    "                   how='inner')\n",
    "\n",
    "translabels.to_csv(\"processed_trans_labels_head.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfe7cb-7345-4d74-bbea-0623b4cf6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = []\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "    path = 'data/IEMOCAP_full_release/'+ sess_name + '/sentences/wav/'\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "    print (sess_name + \", #sum files: \" + str(len(list_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fe17e-97c5-448f-a66f-fcbfaf1eb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('processed_trans_labels_head.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a192c7d-f1fe-4cbe-8720-49f70c88e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rows=len(list_files)\n",
    "# cnt = 0\n",
    "index=0\n",
    "sprectrogram_shape=[]\n",
    "docs = []\n",
    "bookmark=0\n",
    "extraLabel=0\n",
    "for everyFile in list_files:\n",
    "  if(everyFile.split('/')[-1].endswith('.wav')):\n",
    "    filename=everyFile.split('/')[-1].strip('.wav')\n",
    "    lable=df.loc[df['sessionID']==filename]['label'].values[0]\n",
    "    text=df.loc[df['sessionID']==filename]['text'].values[0]\n",
    "    # print('label',lable)\n",
    "    if(lable!=-1):\n",
    "      input_batch = vggish_input.wavfile_to_examples(everyFile)\n",
    "      # print(input_batch.size())\n",
    "      if (len(input_batch.size()) < 4) or (input_batch.size(dim=0) <= 1):\n",
    "        # print(\"Wrong\", input_batch.size())\n",
    "        continue\n",
    "      elif (len(input_batch.size()) == 4) and (input_batch.size(dim=0) > 1):\n",
    "        # print(\"Correct\", input_batch.size())\n",
    "        docs.append({\n",
    "           'fileName':everyFile.split('/')[-1].strip('.wav'),\n",
    "           'text':text,\n",
    "           'sprectrome':input_batch,\n",
    "           'label':lable\n",
    "                })\n",
    "        index+=1\n",
    "        # print('index',index)\n",
    "        # cnt+=1\n",
    "        # if cnt > 100:\n",
    "          # break\n",
    "    else:\n",
    "      extraLabel=extraLabel+1\n",
    "      # print('extraLabel',extraLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857194a1-e42d-466f-8280-a2aa5e3642f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(docs)\n",
    "random.shuffle(docs)\n",
    "random.shuffle(docs)\n",
    "total_length=len(docs)\n",
    "train_length=int(.8*total_length)\n",
    "train_list=docs[0:train_length]\n",
    "test_list=docs[train_length:]\n",
    "print('no of items for train ',len(train_list))\n",
    "print('no of items for test ',len(test_list))\n",
    "# no of items for train  4424\n",
    "# no of items for test  1107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b75414-326d-436a-9049-eb6dbfe2280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "train_file = open(\"train_data.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(train_list, train_file)\n",
    "\n",
    "train_file.close()\n",
    "\n",
    "test_file = open(\"test_data.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(test_list, test_file)\n",
    "\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9b8cb-f3fc-4195-8d9c-ed5b0631ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_file = open(\"train_data.pkl\", \"rb\")\n",
    "\n",
    "train_list = pickle.load(train_file)\n",
    "\n",
    "print(train_list[0])\n",
    "\n",
    "test_file = open(\"test_data.pkl\", \"rb\")\n",
    "\n",
    "test_list = pickle.load(test_file)\n",
    "\n",
    "print(test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb481b-4884-48d5-a1d4-a3956d94d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_text= []\n",
    "def hook_text(module, input, output):\n",
    "    outputs_text.clear()\n",
    "    outputs_text.append(output)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2aa0f6-6a79-44d3-91e5-bd0891373a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Text-only model\n",
    "# class Text_Only(nn.Module):\n",
    "#     def __init__(self, num_classes=4):\n",
    "#         super(MMSER, self).__init__()\n",
    "#         self.num_classes=num_classes\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#         self.text_model= BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes, return_dict=True)        \n",
    "        \n",
    "#         self.text_model.bert.pooler.register_forward_hook(hook_text)\n",
    "        \n",
    "#         for param in self.text_model.parameters():\n",
    "#           param.requires_grad = False\n",
    "\n",
    "#         self.dropout = nn.Dropout(.5)\n",
    "#         self.linear1 = nn.Linear(768, 64)\n",
    "#         self.linear2 = nn.Linear(64, num_classes)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self,text,audio):\n",
    "#         self.text_model(text)\n",
    "#         text_embed=outputs_text[0]\n",
    "#         text_embed = self.linear1(text_embed)\n",
    "#         # print(\"T\", text_embed.shape)\n",
    "#         x=self.dropout(concat_embded)\n",
    "#         x=self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07505795-c6b7-4537-a15f-4c739351cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Audio-only model\n",
    "# class Audio_Only(nn.Module):\n",
    "#     def __init__(self, num_classes=4):\n",
    "#         super(MMSER, self).__init__()\n",
    "#         self.num_classes=num_classes\n",
    "#         self.audio_model= vggish()\n",
    "        \n",
    "#         self.linear1 = nn.Linear(128, 64)\n",
    "#         self.linear2 = nn.Linear(64, num_classes)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self,text,audio):\n",
    "#         audio_embed = self.audio_model(audio)\n",
    "#         # print(\"A1\", audio_embed.shape)\n",
    "#         if (len(audio_embed.size()) == 1):\n",
    "#             audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "#         elif (audio_embed.shape[0] == 1) and (audio_embed.shape[1] == 128):\n",
    "#             audio_embed = audio_embed\n",
    "#         elif (audio_embed.shape[0] > 1) and (audio_embed.shape[1] == 128):\n",
    "#             audio_embed=torch.sum(audio_embed, dim=0)\n",
    "#             audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "#         # print(\"A2\", audio_embed.shape)\n",
    "#         x=self.linear1(audio_embed)\n",
    "#         x=self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908a74d-43e5-443a-9bd3-723b84f032a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-modal model\n",
    "class MMSER(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(MMSER, self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model= BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes, return_dict=True)\n",
    "        self.audio_model= vggish()\n",
    "        \n",
    "        self.text_model.bert.pooler.register_forward_hook(hook_text)\n",
    "        \n",
    "        for param in self.text_model.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.linear1 = nn.Linear(768, 128)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,text,audio):\n",
    "        self.text_model(text)\n",
    "        audio_embed = self.audio_model(audio)\n",
    "        # print(\"A1\", audio_embed.shape)\n",
    "        text_embed=outputs_text[0]\n",
    "        text_embed = self.linear1(text_embed)\n",
    "        # print(\"T\", text_embed.shape)\n",
    "        if (len(audio_embed.size()) == 1):\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        elif (audio_embed.shape[0] == 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed = audio_embed\n",
    "        elif (audio_embed.shape[0] > 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed=torch.sum(audio_embed, dim=0)\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        # print(\"A2\", audio_embed.shape)\n",
    "        concat_embded=torch.cat((text_embed,audio_embed),1)\n",
    "        # print(\"Concatenated\", concat_embded.shape)\n",
    "        x=self.dropout(concat_embded)\n",
    "        x=self.linear2(x)\n",
    "        x=self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff6061-9555-4977-badb-17a866ae1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Text_Only(num_classes=4) # Only text embedding\n",
    "# model = Audio_Only(num_classes=4) # Only audio embedding\n",
    "model=MMSER(num_classes=4) # Multi-modal with both text and audio embeddings\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c2ad-9c2a-497a-bd92-48c9edd1ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the designed model\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# label1=train_list[100]['label']\n",
    "# # label1 = int(label1)\n",
    "# text=train_list[100]['text']\n",
    "# input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "# input_ids=input_ids.to(device)\n",
    "# label1=torch.tensor([label1])\n",
    "# label1=label1.to(device)\n",
    "# sprectrome=train_list[100]['sprectrome']\n",
    "# sprectrome=sprectrome.to(device)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids,sprectrome)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898edafb-4a47-4914-9b8f-1ca5f02eb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "writer = SummaryWriter(log_dir='logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34086bd-fee1-4d65-91ac-61ae2f993fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Text-only model\n",
    "# start_epoch = 0\n",
    "# total_steps = 1\n",
    "# NUM_EPOCHS=101\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model.train()\n",
    "# model.to(device)\n",
    "# for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "#   print(\"*\"*80)\n",
    "#   print(\"Epochs:\", epoch)\n",
    "#   print(\"*\"*80)\n",
    "#   lr_scheduler.step()\n",
    "#   random.shuffle(train_list)\n",
    "#   for every_trainlist in train_list:\n",
    "#     label1=every_trainlist['label']\n",
    "#     label1 = int(label1)\n",
    "#     text=every_trainlist['text']\n",
    "#     label1=torch.tensor([label1])\n",
    "#     optimizer.zero_grad()\n",
    "#     input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0) \n",
    "#     label1=label1.to(device)\n",
    "#     input_ids=input_ids.to(device)\n",
    "#     output = model(input_ids)\n",
    "#     loss = criterion(output, label1)\n",
    "#     print('loss',loss.item())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     _, preds = torch.max(output, 1)\n",
    "#     accuracy = torch.sum(preds == label1)\n",
    "#     print('accuracy.item()',accuracy.item())\n",
    "#     if total_steps % 10 == 0:\n",
    "#       with torch.no_grad():\n",
    "#         _, preds = torch.max(output, 1)\n",
    "#         accuracy = torch.sum(preds == label1)\n",
    "#         writer.add_scalar('loss', loss.item(), total_steps)\n",
    "#         writer.add_scalar('accuracy', accuracy.item(), total_steps)                     \n",
    "#     total_steps+=1\n",
    "#   if epoch % 5 == 0:\n",
    "#     model_version = \"bert_fc768-64_text_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "#     torch.save(model, os.path.join(\"models\", model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52de0e-cf73-49c6-8598-559e6e3834e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Audio-only model\n",
    "# start_epoch = 0\n",
    "# total_steps = 1\n",
    "# NUM_EPOCHS=101\n",
    "# model.train()\n",
    "# model.to(device)\n",
    "# for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "#   print(\"*\"*80)\n",
    "#   print(\"Epochs:\", epoch)\n",
    "#   print(\"*\"*80)\n",
    "#   lr_scheduler.step()\n",
    "#   random.shuffle(train_list)\n",
    "#   for every_trainlist in train_list:\n",
    "#     label1=every_trainlist['label']\n",
    "#     label1 = int(label1)\n",
    "#     label1=torch.tensor([label1])\n",
    "#     sprectrome=every_trainlist['sprectrome']\n",
    "#     if(sprectrome.shape[2]>65):\n",
    "#       optimizer.zero_grad()\n",
    "#       sprectrome = sprectrome.to(device)\n",
    "#       label1=label1.to(device)\n",
    "#       output = model(sprectrome)\n",
    "#       loss = criterion(output, label1)\n",
    "#       print('loss',loss.item())\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "#       _, preds = torch.max(output, 1)\n",
    "#       accuracy = torch.sum(preds == label1)\n",
    "#       print('accuracy.item()',accuracy.item())\n",
    "#       if total_steps % 10 == 0:\n",
    "#         with torch.no_grad():\n",
    "#           _, preds = torch.max(output, 1)\n",
    "#           accuracy = torch.sum(preds == label1)\n",
    "#           writer.add_scalar('loss', loss.item(), total_steps)\n",
    "#           writer.add_scalar('accuracy', accuracy.item(), total_steps)                     \n",
    "#       total_steps+=1\n",
    "#   if epoch % 5 == 0:\n",
    "#     model_version = \"vggish_fc128-64_audio_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "#     torch.save(model, os.path.join(\"models\", model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2255fcd-b33d-45b2-8815-c9165b2aba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multi-modal model\n",
    "start_epoch = 0\n",
    "total_steps = 1\n",
    "NUM_EPOCHS=101\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.train()\n",
    "model.to(device)\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "  print(\"*\"*80)\n",
    "  print(\"Epochs:\", epoch)\n",
    "  print(\"*\"*80)\n",
    "  lr_scheduler.step()\n",
    "  random.shuffle(train_list)\n",
    "  for every_trainlist in train_list:\n",
    "    label1=every_trainlist['label']\n",
    "    label1 = int(label1)\n",
    "    text=every_trainlist['text']\n",
    "    label1=torch.tensor([label1])\n",
    "    sprectrome=every_trainlist['sprectrome']\n",
    "    if(sprectrome.shape[2]>65):\n",
    "      optimizer.zero_grad()\n",
    "      input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0) \n",
    "      sprectrome = sprectrome.to(device)\n",
    "      label1=label1.to(device)\n",
    "      input_ids=input_ids.to(device)\n",
    "      output = model(input_ids,sprectrome)\n",
    "      loss = criterion(output, label1)\n",
    "      print('loss',loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      _, preds = torch.max(output, 1)\n",
    "      accuracy = torch.sum(preds == label1)\n",
    "      print('accuracy.item()',accuracy.item())\n",
    "      if total_steps % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "          _, preds = torch.max(output, 1)\n",
    "          accuracy = torch.sum(preds == label1)\n",
    "          writer.add_scalar('loss', loss.item(), total_steps)\n",
    "          writer.add_scalar('accuracy', accuracy.item(), total_steps)                     \n",
    "      total_steps+=1\n",
    "  if epoch % 5 == 0:\n",
    "    model_version = \"vggish_bert_f4fc364_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "    torch.save(model, os.path.join(\"models\", model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625089f9-021c-4075-9abe-1b90c571893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "# ## Load Text-only model\n",
    "# model_version = \"bert_fc768-64_text_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "# model=torch.load(os.path.join(\"models\", model_version))\n",
    "# ## Load Audio-only model\n",
    "# model_version = \"vggish_fc128-64_audio_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "# model=torch.load(os.path.join(\"models\", model_version))\n",
    "## Load Multi-modal model\n",
    "model_version = \"vggish_bert_f4fc364_{}-{}-{}-{}.pt\".format(epoch, currentMonth, currentDay, currentYear)\n",
    "model=torch.load(os.path.join(\"models\", model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6aec9-200b-44e0-a7bc-15b4d7c9a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text-only prediction\n",
    "# y_actu=[]\n",
    "# y_pred=[]\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# for every_test_list in test_list:\n",
    "#     label1=every_test_list['label']\n",
    "#     label1=torch.tensor([label1])\n",
    "#     label1 = label1.to(device)\n",
    "#     text=every_test_list['text']\n",
    "#     input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "#     input_ids = input_ids.to(device)\n",
    "#     with torch.no_grad():\n",
    "#       output = model(input_ids)\n",
    "#       _, preds = torch.max(output, 1)\n",
    "#       y_actu.append(label1.cpu().numpy()[0])\n",
    "#       y_pred.append(preds.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ae872-93ce-4268-8ebb-da02242497f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Audio-only prediction\n",
    "# y_actu=[]\n",
    "# y_pred=[]\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# for every_test_list in test_list:\n",
    "#     label1=every_test_list['label']\n",
    "#     label1=torch.tensor([label1])\n",
    "#     label1 = label1.to(device)\n",
    "#     sprectrome=every_test_list['sprectrome']\n",
    "#     sprectrome = sprectrome.to(device)\n",
    "#     with torch.no_grad():\n",
    "#       if(sprectrome.shape[2]>65):\n",
    "#         output = model(sprectrome)\n",
    "#         _, preds = torch.max(output, 1)\n",
    "#         y_actu.append(label1.cpu().numpy()[0])\n",
    "#         y_pred.append(preds.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca49ca4-6157-4234-a439-d6ddb1a05090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-modal prediction\n",
    "y_actu=[]\n",
    "y_pred=[]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for every_test_list in test_list:\n",
    "    label1=every_test_list['label']\n",
    "    label1=torch.tensor([label1])\n",
    "    label1 = label1.to(device)\n",
    "    sprectrome=every_test_list['sprectrome']\n",
    "    sprectrome = sprectrome.to(device)\n",
    "    text=every_test_list['text']\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "      if(sprectrome.shape[2]>65):\n",
    "        output = model(input_ids,sprectrome)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        y_actu.append(label1.cpu().numpy()[0])\n",
    "        y_pred.append(preds.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd8e16-37b3-476c-9634-5991d4e12309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_actu, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c626f95-b16d-4819-86a9-55f72f1d612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "sns.heatmap(cmn, cmap='flare', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8818f6-9c2e-4106-923f-34f0983d673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5052140-cecf-4da8-b0ab-66f3b50170f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = balanced_accuracy_score(y_actu, y_pred)\n",
    "print(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11f929-20ca-4952-a393-43152161a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = accuracy_score(y_actu, y_pred)\n",
    "print(ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b83d2-b39e-4442-885f-28d1162a1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
