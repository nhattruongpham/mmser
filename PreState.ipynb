{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0faec3f-1899-4c54-8929-bad602beed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.fftpack import fft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from cca_zoo.deepmodels import DCCA\n",
    "from torchvggish import vggish, vggish_input\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from transformers import BertForSequenceClassification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "# model_urls = {\n",
    "#     'vggish': 'https://github.com/harritaylor/torchvggish/'\n",
    "#               'releases/download/v0.1/vggish-10086976.pth',\n",
    "#     'pca': 'https://github.com/harritaylor/torchvggish/'\n",
    "#            'releases/download/v0.1/vggish_pca_params-970ea276.pth'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfcd0d-abbb-4013-b0a0-7e2f59c88cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def VGGISH(**kwargs):\n",
    "#     model = VGGish(urls=model_urls, **kwargs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a5200-df41-427b-913d-099f6a879917",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dirname        : path that need to be searched\n",
    "ret                : files in the dirname (recursive)\n",
    "list_avoid_dir : dirname need to be skipped\n",
    "usage           : \n",
    "    list_files = []\n",
    "    file_search(dirname, list_files):   \n",
    "'''\n",
    "def file_search(dirname, ret, list_avoid_dir=[]):\n",
    "    \n",
    "    filenames = os.listdir(dirname)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "\n",
    "        if os.path.isdir(full_filename) :\n",
    "            if full_filename.split('/')[-1] in list_avoid_dir:\n",
    "                continue\n",
    "            else:\n",
    "                file_search(full_filename, ret, list_avoid_dir)\n",
    "            \n",
    "        else:\n",
    "            ret.append(full_filename)          \n",
    "\n",
    "            \n",
    "\n",
    "'''\n",
    "filename : filename (inc. path) that will be inspected\n",
    "'''\n",
    "def find_encoding(filename):\n",
    "    rawdata = open(filename, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding']    \n",
    "    return charenc\n",
    "            \n",
    "'''\n",
    "dir_name : dir_name (inc. path) that will be created ( full-path name )\n",
    "'''\n",
    "def create_folder(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e3c45-ce75-4ba4-89d9-2b94b5028f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trans(list_in_file, out_file):\n",
    "    \n",
    "    lines = []\n",
    "    \n",
    "    for in_file in list_in_file:\n",
    "        cnt = 0\n",
    "        \n",
    "        with open(in_file, 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "\n",
    "        with open(out_file, 'a') as f2:\n",
    "\n",
    "            csv_writer = csv.writer(f2)\n",
    "            lines = sorted(lines)                  # sort based on first element\n",
    "            \n",
    "            for line in lines:\n",
    "\n",
    "                name = line.split(':')[0].split(' ')[0].strip()\n",
    "                \n",
    "                # unwanted case \n",
    "                if name[:3] != 'Ses':             # noise transcription such as reply  M: sorry\n",
    "                    continue\n",
    "                elif name[-3:-1] == 'XX':        # we don't have matching pair in label\n",
    "                    continue\n",
    "                trans = line.split(':')[1].strip()\n",
    "                \n",
    "                cnt += 1\n",
    "                csv_writer.writerow([name, trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451084a-f11a-490d-96ef-0f9a750507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [schema] ID, transcriptions [csv]\n",
    "\n",
    "list_files = []\n",
    "\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "\n",
    "    path = '/media/phamnhattruong/ReSe/Datasets/Emotion/IEMOCAP/IEMOCAP_full_release/IEMOCAP_full_release/' + sess_name + '/dialog/transcriptions/'\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "\n",
    "    print (sess_name + \", #sum files: \" + str(len(list_files)))\n",
    "\n",
    "extract_trans(list_files, 'processed_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71653533-f3e3-4683-af05-2343d62b47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents of csv file\n",
    "file = pd.read_csv(\"processed_trans.csv\")\n",
    "  \n",
    "# adding header\n",
    "headerList = ['sessionID', 'text']\n",
    "  \n",
    "# converting data frame to csv\n",
    "file.to_csv(\"processed_trans_head.csv\", header=headerList, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d3c39-97df-4055-bca1-b88e8793e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_category = [\n",
    "                'ang',\n",
    "                'hap',\n",
    "                'sad',\n",
    "                'neu',\n",
    "                'fru',\n",
    "                'exc',\n",
    "                'fea',\n",
    "                'sur',\n",
    "                'dis',\n",
    "                'oth',\n",
    "                'xxx'\n",
    "                ]\n",
    "\n",
    "category = {}\n",
    "for c_type in list_category:\n",
    "    if category.__contains__(c_type):\n",
    "        pass\n",
    "    else:\n",
    "        category[c_type] = len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff6bb3-a08f-4db5-a2ec-d1556043ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_category(lines):\n",
    "    is_target = True\n",
    "    \n",
    "    id = ''\n",
    "    c_label = ''\n",
    "    list_ret = []\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        if is_target == True:\n",
    "            \n",
    "            try:\n",
    "                id          = line.split('\\t')[1].strip()  #  extract ID\n",
    "                c_label  = line.split('\\t')[2].strip()  #  extract category\n",
    "                if not category.__contains__(c_label):\n",
    "                    print(\"ERROR nokey \", c_label)\n",
    "                    sys.exit()\n",
    "                \n",
    "                list_ret.append( [id, c_label] )\n",
    "                is_target = False\n",
    "\n",
    "            except:\n",
    "                print(\"ERROR \", line)\n",
    "                sys.exit()\n",
    "        \n",
    "        else:\n",
    "            if line == '\\n':\n",
    "                is_target = True\n",
    "        \n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4cd04-0517-4e8e-8210-df647b7a5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(list_in_file, out_file) :\n",
    "    id = ''\n",
    "    lines = []\n",
    "    list_ret = []\n",
    "    \n",
    "    for in_file in list_in_file:\n",
    "        \n",
    "        with open(in_file, 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "            lines = lines[2:]                           # remove head\n",
    "            list_ret = find_category(lines)\n",
    "            \n",
    "        list_ret = sorted(list_ret)                   # sort based on first element\n",
    "    \n",
    "        with open(out_file, 'a') as f2:\n",
    "            csv_writer = csv.writer(f2)\n",
    "            csv_writer.writerows(list_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a13d5-99e8-4e28-8834-d5c8e67e2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [schema] ID, label [csv]\n",
    "\n",
    "list_files = []\n",
    "list_avoid_dir = ['Attribute', 'Categorical', 'Self-evaluation']\n",
    "\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "\n",
    "    path = '/media/phamnhattruong/ReSe/Datasets/Emotion/IEMOCAP/IEMOCAP_full_release/IEMOCAP_full_release/' + sess_name + '/dialog/EmoEvaluation/'\n",
    "    file_search(path, list_files, list_avoid_dir)\n",
    "    list_files = sorted(list_files)\n",
    "\n",
    "    print(sess_name + \", #sum files: \" + str(len(list_files)))\n",
    "\n",
    "extract_labels(list_files, \"processed_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565dc88-9a24-45bf-8cf9-293153b798e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents of csv file\n",
    "file = pd.read_csv(\"processed_labels.csv\")\n",
    "  \n",
    "# adding header\n",
    "headerList = ['sessionID', 'label']\n",
    "  \n",
    "# converting data frame to csv\n",
    "file.to_csv(\"processed_labels_head.csv\", header=headerList, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5b862-96fb-4d52-955a-385a388bfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.read_csv('processed_labels_head.csv')\n",
    "dfl.loc[dfl[\"label\"] == \"ang\", \"label\"] = 0\n",
    "dfl.loc[dfl[\"label\"] == \"hap\", \"label\"] = 1\n",
    "dfl.loc[dfl[\"label\"] == \"exc\", \"label\"] = 1\n",
    "dfl.loc[dfl[\"label\"] == \"sad\", \"label\"] = 2\n",
    "dfl.loc[dfl[\"label\"] == \"neu\", \"label\"] = 3\n",
    "dfl.loc[dfl[\"label\"] == \"fru\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"fea\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"sur\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"dis\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"oth\", \"label\"] = -1\n",
    "dfl.loc[dfl[\"label\"] == \"xxx\", \"label\"] = -1\n",
    "dfl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ca0ab-a865-4ae8-8445-6ff5f9f23131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.to_csv(\"processed_digital_labels_head.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22cbe0-97cb-4d09-8657-4fff2f412229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading two csv files\n",
    "data1 = pd.read_csv('processed_trans_head.csv')\n",
    "data2 = pd.read_csv('processed_digital_labels_head.csv')\n",
    "  \n",
    "# using merge function by setting how='inner'\n",
    "translabels = pd.merge(data1, data2, \n",
    "                   on='sessionID', \n",
    "                   how='inner')\n",
    "\n",
    "translabels.to_csv(\"processed_trans_labels_head.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfe7cb-7345-4d74-bbea-0623b4cf6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = []\n",
    "for x in range(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "    path = '/media/phamnhattruong/ReSe/Datasets/Emotion/IEMOCAP/IEMOCAP_full_release/IEMOCAP_full_release/'+ sess_name + '/sentences/wav/'\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "    print (sess_name + \", #sum files: \" + str(len(list_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dcf7b-40c4-4ab6-be5b-1c45426317ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def audio2spectrogram(filepath):\n",
    "#     #fig = plt.figure(figsize=(5,5))\n",
    "#     samplerate, test_sound  = wavfile.read(filepath,mmap=True)\n",
    "#     #print('samplerate',samplerate)\n",
    "#     _, spectrogram = log_specgram(test_sound, samplerate)\n",
    "#     #print(spectrogram.shape)\n",
    "#     #print(type(spectrogram))\n",
    "#     #plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "#     return spectrogram\n",
    "    \n",
    "# def audio2wave(filepath):\n",
    "#     fig = plt.figure(figsize=(5,5))\n",
    "#     samplerate, test_sound  = wavfile.read(filepath,mmap=True)\n",
    "#     plt.plot(test_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564730a0-bb5f-4aab-b34e-8880dbcc5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_specgram(audio, sample_rate, window_size=40,\n",
    "#                  step_size=20, eps=1e-10):\n",
    "#     nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "#     noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "#     #print('noverlap',noverlap)\n",
    "#     #print('nperseg',nperseg)\n",
    "#     freqs, _, spec = signal.spectrogram(audio,\n",
    "#                                     fs=sample_rate,\n",
    "#                                     window='hann',\n",
    "#                                     nperseg=nperseg,\n",
    "#                                     noverlap=noverlap,\n",
    "#                                     detrend=False)\n",
    "#     return freqs, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9417715-47bb-4088-b68a-69c7f7f50451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # N_CHANNELS = 3\n",
    "# def get_3d_spec(Sxx_in, moments=None):\n",
    "#     if moments is not None:\n",
    "#         (base_mean, base_std, delta_mean, delta_std,\n",
    "#              delta2_mean, delta2_std) = moments\n",
    "#     else:\n",
    "#         base_mean, delta_mean, delta2_mean = (0, 0, 0)\n",
    "#         base_std, delta_std, delta2_std = (1, 1, 1)\n",
    "#     h, w = Sxx_in.shape\n",
    "#     right1 = np.concatenate([Sxx_in[:, 0].reshape((h, -1)), Sxx_in], axis=1)[:, :-1]\n",
    "#     delta = (Sxx_in - right1)[:, 1:]\n",
    "#     delta_pad = delta[:, 0].reshape((h, -1))\n",
    "#     delta = np.concatenate([delta_pad, delta], axis=1)\n",
    "#     right2 = np.concatenate([delta[:, 0].reshape((h, -1)), delta], axis=1)[:, :-1]\n",
    "#     delta2 = (delta - right2)[:, 1:]\n",
    "#     delta2_pad = delta2[:, 0].reshape((h, -1))\n",
    "#     delta2 = np.concatenate([delta2_pad, delta2], axis=1)\n",
    "#     base = (Sxx_in - base_mean) / base_std\n",
    "#     delta = (delta - delta_mean) / delta_std\n",
    "#     delta2 = (delta2 - delta2_mean) / delta2_std\n",
    "#     stacked = [arr.reshape((h, w, 1)) for arr in (base, delta, delta2)]\n",
    "#     return np.concatenate(stacked, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fe17e-97c5-448f-a66f-fcbfaf1eb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('processed_trans_labels_head.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93476388-44aa-4a1a-84ab-cf8276cae3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['label'] = df['label'].replace(['-1','0','1', '2', '3'],[-1,0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e98e-5249-43af-a90a-14c48cbc2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a192c7d-f1fe-4cbe-8720-49f70c88e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_rows=len(list_files)\n",
    "# # cnt = 0\n",
    "# index=0\n",
    "# sprectrogram_shape=[]\n",
    "# docs = []\n",
    "# bookmark=0\n",
    "# extraLabel=0\n",
    "# for everyFile in list_files:\n",
    "#   if(everyFile.split('/')[-1].endswith('.wav')):\n",
    "#     filename=everyFile.split('/')[-1].strip('.wav')\n",
    "#     lable=df.loc[df['sessionID']==filename]['label'].values[0]\n",
    "#     text=df.loc[df['sessionID']==filename]['text'].values[0]\n",
    "#     # print('label',lable)\n",
    "#     if(lable!=-1):\n",
    "#       #sprectrogram_shape.append(audio2spectrogram(everyFile))\n",
    "#       # spector=audio2spectrogram(everyFile)\n",
    "#       # spector=get_3d_spec(spector)\n",
    "#       # npimg = np.transpose(spector,(2,0,1))\n",
    "#       # input_tensor=torch.tensor(npimg)\n",
    "#       # input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "#       input_batch = vggish_input.wavfile_to_examples(everyFile)\n",
    "#       # print(input_batch.size())\n",
    "#       if (len(input_batch.size()) < 4) or (input_batch.size(dim=0) <= 1):\n",
    "#         # print(\"Wrong\", input_batch.size())\n",
    "#         continue\n",
    "#       #X, sample_rate = librosa.load(everyFile, res_type='kaiser_fast',sr=22050*2)\n",
    "#       #sample_rate = np.array(sample_rate)\n",
    "#       #mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=13),axis=0)\n",
    "#       #feature = mfccs\n",
    "#       elif (len(input_batch.size()) == 4) and (input_batch.size(dim=0) > 1):\n",
    "#         # print(\"Correct\", input_batch.size())\n",
    "#         docs.append({\n",
    "#            'fileName':everyFile.split('/')[-1].strip('.wav'),\n",
    "#            'text':text,\n",
    "#            'sprectrome':input_batch,\n",
    "#            'label':lable\n",
    "#                 })\n",
    "#         index+=1\n",
    "#         # print('index',index)\n",
    "#         # cnt+=1\n",
    "#         # if cnt > 100:\n",
    "#           # break\n",
    "#     else:\n",
    "#       extraLabel=extraLabel+1\n",
    "#       # print('extraLabel',extraLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857194a1-e42d-466f-8280-a2aa5e3642f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(docs)\n",
    "# random.shuffle(docs)\n",
    "# random.shuffle(docs)\n",
    "# total_length=len(docs)\n",
    "# train_length=int(.8*total_length)\n",
    "# train_list=docs[0:train_length]\n",
    "# test_list=docs[train_length:]\n",
    "# print('no of items for train ',len(train_list))\n",
    "# print('no of items for test ',len(test_list))\n",
    "# # no of items for train  4424\n",
    "# # no of items for test  1107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a895741-2845-459e-a847-b3eeee7224dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b75414-326d-436a-9049-eb6dbfe2280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = open(\"train_data.pkl\", \"wb\")\n",
    "\n",
    "# pickle.dump(train_list, train_file)\n",
    "\n",
    "# train_file.close()\n",
    "\n",
    "# test_file = open(\"test_data.pkl\", \"wb\")\n",
    "\n",
    "# pickle.dump(test_list, test_file)\n",
    "\n",
    "# test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9b8cb-f3fc-4195-8d9c-ed5b0631ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"train_data.pkl\", \"rb\")\n",
    "\n",
    "train_list = pickle.load(train_file)\n",
    "\n",
    "print(train_list[0])\n",
    "\n",
    "test_file = open(\"test_data.pkl\", \"rb\")\n",
    "\n",
    "test_list = pickle.load(test_file)\n",
    "\n",
    "print(test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb481b-4884-48d5-a1d4-a3956d94d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_text= []\n",
    "def hook_text(module, input, output):\n",
    "    outputs_text.clear()\n",
    "    outputs_text.append(output)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cccac-2263-4ade-96cf-f6a1a55a4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_audio= []\n",
    "def hook_audio(module, input, output):\n",
    "    outputs_audio.clear()\n",
    "    outputs_audio.append(output)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908a74d-43e5-443a-9bd3-723b84f032a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMSER(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(MMSER, self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model= BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes, return_dict=True)\n",
    "        self.audio_model= vggish()\n",
    "        \n",
    "        \n",
    "        self.text_model.bert.pooler.register_forward_hook(hook_text)\n",
    "        # self.audio_model.features.register_forward_hook(hook_audio)\n",
    "        \n",
    "        for param in self.text_model.parameters():\n",
    "          param.requires_grad = False\n",
    "        # for param in self.audio_model.parameters():\n",
    "        #   param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.linear1 = nn.Linear(768, 128)\n",
    "        self.linear2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,text,audio):\n",
    "        self.text_model(text)\n",
    "        # self.audio_model(audio)\n",
    "        # audio_embed=outputs_audio[0]\n",
    "        audio_embed = self.audio_model(audio)\n",
    "        print(\"A1\", audio_embed.shape)\n",
    "        text_embed=outputs_text[0]\n",
    "        text_embed = self.linear1(text_embed)\n",
    "        print(\"T\", text_embed.shape)\n",
    "        # audio_embed=torch.flatten(audio_embed, start_dim=2)#a1,a2,a3......al{a of dim c} \n",
    "        if (len(audio_embed.size()) == 1):\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        elif (audio_embed.shape[0] == 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed = audio_embed\n",
    "        elif (audio_embed.shape[0] > 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed=torch.sum(audio_embed, dim=0)\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        print(\"A2\", audio_embed.shape)\n",
    "        concat_embded=torch.cat((text_embed,audio_embed),1)\n",
    "        print(\"Concatenated\", concat_embded.shape)\n",
    "        x=self.dropout(concat_embded)\n",
    "        x=self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff6061-9555-4977-badb-17a866ae1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MMSER(num_classes=4)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a121523-7e41-477f-919b-b887a3460b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#       if(param.requires_grad):\n",
    "#         print(name)\n",
    "#       else:\n",
    "#         print('no grad',name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243c2ad-9c2a-497a-bd92-48c9edd1ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# label1=train_list[100]['label']\n",
    "# # label1 = int(label1)\n",
    "# text=train_list[100]['text']\n",
    "# input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "# input_ids=input_ids.to(device)\n",
    "# label1=torch.tensor([label1])\n",
    "# label1=label1.to(device)\n",
    "# sprectrome=train_list[100]['sprectrome']\n",
    "# sprectrome=sprectrome.to(device)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids,sprectrome)\n",
    "#     #output.squeeze().shape\n",
    "#     #output=torch.flatten(output, start_dim=2)\n",
    "#     #print(output.shape)\n",
    "#     #output=torch.sum(output, dim=2)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898edafb-4a47-4914-9b8f-1ca5f02eb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter(log_dir='logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52de0e-cf73-49c6-8598-559e6e3834e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 1\n",
    "NUM_EPOCHS=5\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.train()\n",
    "model.to(device)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  lr_scheduler.step()\n",
    "  random.shuffle(train_list)\n",
    "  for every_trainlist in train_list:\n",
    "    label1=every_trainlist['label']\n",
    "    label1 = int(label1)\n",
    "    text=every_trainlist['text']\n",
    "    label1=torch.tensor([label1])\n",
    "    sprectrome=every_trainlist['sprectrome']\n",
    "    if(sprectrome.shape[2]>65):\n",
    "      optimizer.zero_grad()\n",
    "      input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0) \n",
    "      sprectrome = sprectrome.to(device)\n",
    "      label1=label1.to(device)\n",
    "      input_ids=input_ids.to(device)\n",
    "      output = model(input_ids,sprectrome)\n",
    "      #print('softmax output ',output)\n",
    "      loss = criterion(output, label1)\n",
    "      #print('label1',label1)\n",
    "      print('loss',loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      _, preds = torch.max(output, 1)\n",
    "      accuracy = torch.sum(preds == label1)\n",
    "      print('accuracy.item()',accuracy.item())\n",
    "      #print('preds',preds)\n",
    "      if total_steps % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "          _, preds = torch.max(output, 1)\n",
    "          accuracy = torch.sum(preds == label1)\n",
    "          #print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'.format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
    "          writer.add_scalar('loss', loss.item(), total_steps)\n",
    "          writer.add_scalar('accuracy', accuracy.item(), total_steps)                     \n",
    "      total_steps+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656eda7-4873-45c6-bd51-f366088f8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'vggish_bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625089f9-021c-4075-9abe-1b90c571893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('vggish_bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b971029-bf7a-4bbd-ba12-9465fdb3abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu=[]\n",
    "y_pred=[]\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for every_test_list in test_list:\n",
    "    label1=every_test_list['label']\n",
    "    label1=torch.tensor([label1])\n",
    "    label1 = label1.to(device)\n",
    "    sprectrome=every_test_list['sprectrome']\n",
    "    sprectrome = sprectrome.to(device)\n",
    "    text=every_test_list['text']\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "      if(sprectrome.shape[2]>65):\n",
    "        #sprectrome = sprectrome.to('cuda')\n",
    "        #label1=label1.to('cuda')\n",
    "        output = model(input_ids,sprectrome)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        y_actu.append(label1.cpu().numpy()[0])\n",
    "        y_pred.append(preds.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe992fa-62a5-485f-ab6e-c706ec42d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_actu, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e171b-2a92-4090-911b-fd32a5b6f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib\n",
    "# url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n",
    "# try: urllib.URLopener().retrieve(url, filename)\n",
    "# except: urllib.request.urlretrieve(url, filename)\n",
    "embedding_model = vggish()\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()\n",
    "example = vggish_input.wavfile_to_examples(\"bus_chatter.wav\")\n",
    "example = example.to(device)\n",
    "embeddings = embedding_model.forward(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6abca8-5c35-4235-8192-88c861fd631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1a8f6-cfb7-4220-8749-f851ad47c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_embed=torch.flatten(embeddings, start_dim=0)#a1,a2,a3......al{a of dim c} \n",
    "# audio_embed=torch.sum(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb2c09-e29d-4f61-bf47-3692ee44fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36eb0c-21e0-4a82-874d-e4900ba01113",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed = torch.empty(1, 128)\n",
    "audio_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd3e83-8859-4e87-a811-939d3fb4cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio_embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9596c-9226-466d-9051-f6798fd36dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A1\", audio_embed.shape)\n",
    "if (len(audio_embed.size()) == 1):\n",
    "    audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "elif (audio_embed.shape[0] == 1) and (audio_embed.shape[1] == 128):\n",
    "    audio_embed = audio_embed\n",
    "elif (audio_embed.shape[0] > 1) and (audio_embed.shape[1] == 128):\n",
    "    audio_embed=torch.sum(audio_embed, dim=0)\n",
    "    audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "print(\"A2\", audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30be4f-e53c-4bdd-8a98-9a8e4b368a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c8559-ee73-40d2-a21a-2150a54bff76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c0e0d-4fcf-44f7-95aa-04df681e6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMSER_DCCA(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(MMSER_DCCA, self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_model= BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes, return_dict=True)\n",
    "        self.audio_model= vggish()\n",
    "        \n",
    "        \n",
    "        self.text_model.bert.pooler.register_forward_hook(hook_text)\n",
    "        # self.audio_model.features.register_forward_hook(hook_audio)\n",
    "        \n",
    "        for param in self.text_model.parameters():\n",
    "          param.requires_grad = False\n",
    "        # for param in self.audio_model.parameters():\n",
    "        #   param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.linear1 = nn.Linear(768, 128)\n",
    "        self.linear2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,text,audio):\n",
    "        self.text_model(text)\n",
    "        # self.audio_model(audio)\n",
    "        # audio_embed=outputs_audio[0]\n",
    "        audio_embed = self.audio_model(audio)\n",
    "        print(\"A1\", audio_embed.shape)\n",
    "        text_embed=outputs_text[0]\n",
    "        text_embed = self.linear1(text_embed)\n",
    "        print(\"T\", text_embed.shape)\n",
    "        # audio_embed=torch.flatten(audio_embed, start_dim=2)#a1,a2,a3......al{a of dim c} \n",
    "        if (len(audio_embed.size()) == 1):\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        elif (audio_embed.shape[0] == 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed = audio_embed\n",
    "        elif (audio_embed.shape[0] > 1) and (audio_embed.shape[1] == 128):\n",
    "            audio_embed=torch.sum(audio_embed, dim=0)\n",
    "            audio_embed = torch.unsqueeze(audio_embed, dim=0)\n",
    "        print(\"A2\", audio_embed.shape)\n",
    "        dcca = DCCA(latent_dims=2, encoders=[text_embed, audio_embed])\n",
    "        return dcca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9fffc-4021-4cbd-86e7-f552c2965198",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcca_model = MMSER_DCCA(num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b83d2-b39e-4442-885f-28d1162a1557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
